{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2oP1uun77cIh"
   },
   "source": [
    "# Introduction to Natural Language Processing\n",
    "# Name: Sai Krishna Lakshminarayanan Student ID: 18230229\n",
    "# Assignment 1\n",
    "\n",
    "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
    "\n",
    "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
    "\n",
    "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
    "\n",
    "    My siter|sister go|goes to Tonbury .\n",
    "    \n",
    "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
    "\n",
    "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
    "\n",
    "    My Mum goes out some_times|sometimes .\n",
    "    \n",
    "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token.\n",
    "\n",
    "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIVCSJV-7kDs"
   },
   "source": [
    "## Task 1 (10 Marks)\n",
    "\n",
    "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
    "\n",
    "Then split your data into a test set of 100 lines and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RznCZ1mw7mfk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk #importing the required pacakages\n",
    "from nltk.util import ngrams\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(sent):  #defining parsing function to split the given sentence\n",
    "    original_line = [] #this is going to be the original line\n",
    "    corrected_line = [] #this will be the corrected line\n",
    "    indexes = [] #index of the word error in the given sentence\n",
    "    count = 0 \n",
    "    \n",
    "    for i in sent:\n",
    "        if '|' in i:#when there is error\n",
    "            str1 = i.split('|')# Splitting the sentence based on '|'\n",
    "            original_line.append(str1[0])# Previous word to '|' \n",
    "            corrected_line.append(str1[1])# Next word to '|' \n",
    "            indexes.append(count)# index of error\n",
    "        \n",
    "        else:#when no error\n",
    "            original_line.append(i)\n",
    "            corrected_line.append(i)\n",
    "        count = count+1   \n",
    "    final = {'original': original_line, 'corrected': corrected_line, 'indexes': indexes}\n",
    "    \n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(): # to perform perprocess to get data in an executable format\n",
    "    data = []\n",
    "    text_file = open(\"holbrook.txt\", \"r\")#reading the given text file\n",
    "    lines = []\n",
    "    for i in text_file:\n",
    "        lines.append(i.strip())\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in lines]#word tokenization of the given sentence\n",
    "    for sent in sentences:\n",
    "        data.append(parsing(sent))# parse the given sentence and to correct it \n",
    "    \n",
    "    return data\n",
    "\n",
    "data = preprocess()#executing the given text file data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRSX4I0H7pSC"
   },
   "source": [
    "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kt9aR2Gy7p1C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], 'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], 'indexes': [9]}\n"
     ]
    }
   ],
   "source": [
    "#performing cross check as given in the question\n",
    "print(data[2])\n",
    "assert(data[2] == {\n",
    " 'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'],\n",
    " 'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'],\n",
    " 'indexes': [9]\n",
    "})\n",
    "\n",
    "# Splitting the data as given in the question\n",
    "test = data[:100]\n",
    "train = data[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hm5JL7cH7sLK"
   },
   "source": [
    "## **Task 2** (10 Marks): \n",
    "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
    "\n",
    "*Hint: use `Counter` to implement this so it may be called many times*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ge0uHS-7uEK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"unigram('me')==\", 87)\n"
     ]
    }
   ],
   "source": [
    "corrected_train = [elem['corrected'] for elem in train]# taking the values of corrected alone from the train\n",
    "original_test = [elem['original'] for elem in test]#taking the values of original alone from the test\n",
    "corrected_test = [elem['corrected'] for elem in test] #taking the values of corrected alone from the test\n",
    "from collections import Counter\n",
    "def unigram(word):#Unigram function to return the frequency of given word\n",
    "    a = []\n",
    "    word = word.lower()#lower casing the given word\n",
    "    for i in corrected_train:#checking each line of corrected train\n",
    "        a.append(\" \".join(i).lower())#lower casing it\n",
    "    a = \" \".join(a)    \n",
    "    a = nltk.word_tokenize(a)# word tokenization on a\n",
    "    unigram_frquency = nltk.FreqDist(a) # getting the unigram frquency\n",
    "    return unigram_frquency[word]\n",
    "print(\"unigram('me')==\", unigram('me')) #desired result\n",
    "assert unigram('me')==87 #cross verification of the unigram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0039891787794\n"
     ]
    }
   ],
   "source": [
    "def prob(word): # to obtain the probability of the unigram word\n",
    "    b = unigram(word) #getting the frequency of the given word\n",
    "    a=0\n",
    "    for i in corrected_train:\n",
    "        a=a+len(i)#total number of words present in the given corrected train\n",
    "    \n",
    "    probability=float(b)/float(a)#required unigram probability\n",
    "    return probability\n",
    "print(prob('me'))#desired result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w8r8QYj78GPK"
   },
   "source": [
    "## **Task 3** (15 Marks): \n",
    "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SV9Mu8P38IQE",
    "outputId": "9f29e22b-0f8b-4b92-9d5f-fcde3efec970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# Edit distance returns the number of changes to transform one word to another\n",
    "print(edit_distance(\"hello\", \"hi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hm46Lbiz8K8M"
   },
   "source": [
    "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
    "\n",
    "1. Collect the set of all unique tokens in `train`\n",
    "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
    "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
    "\n",
    "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoilAmFW8PCb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mind', 'mine']\n"
     ]
    }
   ],
   "source": [
    "def get_candidates(token):# to replace the given misspelled word through suggestion by means of edit distance\n",
    "    \n",
    "    a = []\n",
    "    \n",
    "    for i in corrected_train:\n",
    "        a.append(\" \".join(i).lower())#lower casing all the words in corrected_train\n",
    "\n",
    "    a = \" \".join(a)\n",
    "    a = nltk.word_tokenize(a)\n",
    "    unigram_frequency = nltk.FreqDist(a)#obtaining the frequency of the words in the given list\n",
    "    unique_words = list(unigram_frequency.keys()) # getting each unique word corresponding to it by help of keys\n",
    "\n",
    "   \n",
    "    s = []\n",
    "    for i in unique_words:# checking every unique word\n",
    "        if edit_distance(i, token)==1 : #when the distance between the given misspelled word and suggestion is 1\n",
    "            s.append(\"\".join(i))\n",
    "    \n",
    "  \n",
    "    return s #desired output\n",
    "print(get_candidates(\"minde\"))\n",
    "assert get_candidates(\"minde\") == ['mind', 'mine'] # cross verification of the get_candidate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGY-eCkN8TIM"
   },
   "source": [
    "## Task 4 (15 Marks):\n",
    "\n",
    "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
    "\n",
    "*Your solution to this should involve `get_candidates`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIGKE4_P8WGP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'white', 'cat']\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "\n",
    "for i in corrected_train: #checking every line of the given corrected train\n",
    "    a.append(\" \".join(i).lower())#lower casing all of them\n",
    "\n",
    "a = \" \".join(a)\n",
    "a = nltk.word_tokenize(a)# word tokenization of the corrected train\n",
    "unigram_frequency = nltk.FreqDist(a) # obtaining the unigram frequency of the words in corrected train\n",
    "unique_words = list(unigram_frequency.keys()) # list of unique words present in the corrected train\n",
    "p=[] # to store unigram probability\n",
    "\n",
    "def correct(sentence): # function to correct the given sentence \n",
    "    corrected = [] #to store the corrected sentence\n",
    "    count = 0\n",
    "    indexes = [] #to show the index of the misspelled word to get the suggestions\n",
    "    for i in sentence: #checking every word in the given sentence\n",
    "        if i.lower() not in unique_words:# when the given word is not present in the unique words list of corrected train\n",
    "            indexes.append(count)\n",
    "            if len(get_candidates(i)) > 1:# when there are two or more suggestions available to a given misspelled word\n",
    "\n",
    "                suggestion = get_candidates(i)#obtaining the suggestions\n",
    "                p.append(prob(i))# getting the unigram probability for all the suggested words\n",
    "                corrected.append(suggestion[p.index(max(p))])#returning the suggested word as the one with maximum probability\n",
    "            else:\n",
    "                corrected.append(\"\".join(get_candidates(i)))# directly obtain the suggestion for the misspelled word \n",
    "\n",
    "        else:\n",
    "            corrected.append(i)# when no correction is needed\n",
    "        count = count+1\n",
    "    return corrected #returning the corrected sentence\n",
    "\n",
    "\n",
    "print(correct([\"this\",\"whitr\",\"cat\"]))#desired output\n",
    "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])#cross verification of the correct function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oG7jC6au8kka"
   },
   "source": [
    "## **Task 5** (10 Marks): \n",
    "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HSXTQypR8mdR",
    "outputId": "853813e4-d71b-42a7-8e96-68d038457628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy % of the algorithm for the test data is   82.653182339\n"
     ]
    }
   ],
   "source": [
    "def accuracy(test):# to determine the accuracy of the algorithm  \n",
    "    correctedSentences = []\n",
    "    ans=0\n",
    "    for line in original_test:#checking every line of the original test\n",
    "        cor = correct(line)# correcting the wrong ones using the correct function\n",
    "        correctedSentences.append(cor) #storing it inthe correctedSentences list\n",
    "    accuracy = []# to store the accuracy of the mechanism\n",
    "    for i in range(len(corrected_test)):#to perform it till the length of the corrected test\n",
    "        count = 0\n",
    "        for index in range(len(corrected_test[i])):\n",
    "            if(correctedSentences[i][index].lower() == corrected_test[i][index].lower()):\n",
    "                #when sentence corrected by correct function and corrected sentence in the test are equal\n",
    "                count += 1    \n",
    "        accuracy.append(float(count)/len(corrected_test[i]))#storing the accuracy for each instances\n",
    "    ans=float(sum(accuracy)/len(accuracy))*100#calculating the overall accuracy for test data\n",
    "    return (ans)\n",
    "\n",
    "print \"The accuracy % of the algorithm for the test data is  \",accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9b-r2JzD8_Zh"
   },
   "source": [
    "## **Task 6 (35 Marks):**\n",
    "\n",
    "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
    "\n",
    "* You may resources beyond those provided here.\n",
    "* You must **not use the test data** in this task.\n",
    "* Provide a short text describing what you intend to do and why. \n",
    "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
    "* Your implementation should not consist of more than 50 lines of code\n",
    "\n",
    "Please note this task is marked according to: demonstration of knowledge from the lecutures (10), originality and appropriateness of solution (10), completeness of description (10) and technical correctness (5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm for sentence detection can be improved in various ways. Here , we considered only unigram probability and did suggestion and correction based on that. But this can be improved inorder to get better accuracy and an enhanced algorithm.Now let us check the ways in which it can be helped to imrpoved\n",
    "\n",
    "here are various ways to improve the accuracy of the algorithm like tense detection, Named entity detection, Bigrams, trigrams, smoothing , Noise Free Corpus etc .  A few methods are described below : \n",
    "\n",
    "1 Checking the kind of error user makes[implemented]\n",
    "\n",
    "*  The user sometimes forgets to provide strokes properly and there occurs punctuation errors. They donot help in improving the efficiency of the code\n",
    "*  These punctuations can be simply ignored inorder to get better accuracy for the algorithm\n",
    "\n",
    "2 Bigram[implemented]\n",
    "\n",
    "* Unigram is implemented in the algorithm. When it is known that 2 words come together usually then it is advisable to perform bigram frequency on it and find its probability to suggest words which will help in executing two words in place of one at a time.This helps in increasing the accuracy of the algorithm.\n",
    "\n",
    "3 Trigram\n",
    "\n",
    "* Trigram probability can also be implemented . This will come in handy when the given sentence is very long and unigram results in consumption of alot of time. So, trigram will help in checking three words at a time and there by reducing the time and as a result increasing the efficiency of the algorithm.\n",
    "\n",
    "\n",
    "4 Part of Speech Tag\n",
    "\n",
    "* Part of Speech Tag provides tagging of each word in the given data.\n",
    "* This helps in training the data set better inorder to understand each word based on its position and whether it is a noun or a verb etc.\n",
    "* This coupled up in suggestion function helps in giving out better yielding suggested words to the misspelled words thereby increasing the accuracy of the algorithm\n",
    "\n",
    "5  Inverse Document Frequency\n",
    "\n",
    "* There are some words that occur frequently in sentences or in data like the stopwords.\n",
    "* When the term frequencies for such words are down scaled, it helps in the betterment of the performance\n",
    "* This inverse document frequency helps in determining how important a word is in a given data. This helps us to ignore the least important ones which will help in increasing the accuracy.\n",
    "\n",
    "6 Large Stopword list \n",
    "\n",
    "* Stopwords hold no value in sentence processing.\n",
    "* So, analysing them and having an elaborate list of stopwords like a,the etc will help us in creating a function to ignore them when they occur.\n",
    "* This reduces the words to be processed thereby increasing the accuracy\n",
    "\n",
    "7 Ignoring least Frequency Words\n",
    "\n",
    "* There will be inert words in any given data will occur only once or so.\n",
    "* They dont help in improving the training the data and just results in increase of the count.\n",
    "* When these words are excluded, the count gets reduced thereby increasing the accuracy\n",
    "\n",
    "8  Name Entity Recognition\n",
    "\n",
    "* The algorithm may not be able to understand a name directly.\n",
    "* Therefore, it is important to train the data with names of the people and objects so that it doesn't consider them to be any junk value and gives out proper result.\n",
    "* This process is very important and improves the accuracy of the algorithm greatly.\n",
    "\n",
    "9 Lemmatization\n",
    "\n",
    "* A word occurs in various forms and tenses in the given data.\n",
    "* This makes the training to be provided in such a way that each word occuring in various forms and tenses to be different.\n",
    "* Therefore, when lemmatization is done, it condenses them to their base word format.\n",
    "* From the base word, it is easy to get the suggestion out for the given one.\n",
    "* This helps in providing training in an efficient way with reduced count of words to be considered.\n",
    "* This increases the accuracy of the algorithm greatly.\n",
    "\n",
    "10  Smoothing\n",
    "\n",
    "* Smoothing can be done to understand the functioning of the algorithm in visual manner also.\n",
    "* It can be done in several ways so that probability is not null whenever execution is made.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "corrected_train = [elem['corrected'] for elem in test]\n",
    "original_test = [elem['original'] for elem in test]\n",
    "    \n",
    "    # Removing all special characters from the list.\n",
    "original_test = [tokenizer.tokenize(\" \".join(elem)) for elem in original_test]\n",
    "corrected_train = [tokenizer.tokenize(\" \".join(elem)) for elem in corrected_train]\n",
    "corrected_train = [tokenizer.tokenize(\" \".join(elem)) for elem in corrected_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram('you are')== 1\n"
     ]
    }
   ],
   "source": [
    "def bigram(words): #function to get the bigram frequency of the given words\n",
    "    a = []\n",
    "    \n",
    "    # This function get words in string, hence converting string of 2 words to tuple.\n",
    "    words = words.split(\" \")#splitting the words\n",
    "    words[0] = words[0].lower()#lower casing the first word\n",
    "    words[1] = words[1].lower()#lower casing the second word\n",
    "    words = tuple(words) #creating tuple so that it cannot be changed\n",
    "    for i in corrected_train: # checking every line in corrected train\n",
    "        a.append(\" \".join(i))\n",
    "        \n",
    "    a = \" \".join(a)    \n",
    "    a = a.lower()#lower casing them\n",
    "    \n",
    "    #Calculating Bigrams for given words.\n",
    "    tokens = nltk.wordpunct_tokenize(a) #tokenizing a based on wordpunct\n",
    "    bigrams=nltk.collocations.BigramCollocationFinder.from_words(tokens)#getting the bigrams from collacation finder\n",
    "    bigram_frequency = dict(bigrams.ngram_fd)#obtaining the bigram frquency of the words\n",
    "    \n",
    "    return bigram_frequency[words]\n",
    "\n",
    "print \"bigram('you are')==\",bigram(\"you are\")#desired output\n",
    "assert bigram(\"you are\")==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLzaC6D28sK9"
   },
   "source": [
    "## **Task 7 (5 Marks):**\n",
    "\n",
    "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4\n",
    "\n",
    "On repeating the task 5 for the changes implemented as in task 6, it is seen that accuracy as improved to an extent thereby increasing the efficiency of the modified alogirithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hw6PzwWn7iEo"
   },
   "outputs": [],
   "source": [
    "def accuracy(test):# to determine the accuracy of the algorithm  \n",
    "    correctedSentences = []\n",
    "    ans=0\n",
    "    for line in original_test:#checking every line of the original test\n",
    "        cor = correct(line)# correcting the wrong ones using the correct function\n",
    "        correctedSentences.append(cor) #storing it inthe correctedSentences list\n",
    "    accuracy = []# to store the accuracy of the mechanism\n",
    "    for i in range(len(corrected_test)):#to perform it till the length of the corrected test\n",
    "        count = 0\n",
    "        for index in range(len(corrected_test[i])):\n",
    "            if(correctedSentences[i][index].lower() == corrected_test[i][index].lower()):\n",
    "                #when sentence corrected by correct function and corrected sentence in the test are equal\n",
    "                count += 1    \n",
    "        accuracy.append(float(count)/len(corrected_test[i]))#storing the accuracy for each instances\n",
    "    ans=float(sum(accuracy)/len(accuracy))*100#calculating the overall accuracy for test data\n",
    "    return (ans)\n",
    "\n",
    "print \"The accuracy % of the algorithm for the test data is  \",accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP 18/19 Assignment 1",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
